\chapter{Theory}

% TODO: what the theory is about
% TODO: photogrammetry vs lidar

\section{Feature-based photogrammetry}
Broadly speaking, photogrammetry is the process of obtaining information about objects (namely their model) using images.
Because the problem is difficult and involves a lot of steps and specialized algorithms, many open-source and proprietary programs have been developed to simplify this task.

The thesis uses the Agisoft Metashape software due to its highly configurable API.
Since it is proprietary and closed-source, there is no easy way to describe and cite the exact algorithms used.
However, a forum statement from the lead developer Dmitry Semyonov \parencite{metashapeForumPost} outlined the general methods used, which this section aims to cover.

Starting with a set of images and ending with a textured model, the process can be split into the following parts:

\begin{enumerate}
	\item \textbf{feature extraction:} for each image, find features that are stable under linear and affine transformations, 3D viewpoint change and illumination
	\item \textbf{feature matching:} match the features across multiple images
	\item \textbf{bundle adjustment:} find the approximate positions of the camera and the matched features (obtaining „structure from motion“ data)
	\item \textbf{dense pointcloud:} generate additional features for higher model quality
	\item \textbf{surface reconstruction:} construct the surface of the model
	\item \textbf{texturing:} apply texture on the constructed model
\end{enumerate}

The following sections aim to cover each of these in part.

\subsection{Feature extraction}
The image features are extracted using the Scale Invariant Feature Transform (SIFT) method described in \citet{lowe1999object,lowe2004distinctive,snavely2008modeling}.
Each of the features is a vector, invariant or partially invariant to linear and affine transformations, illuminance change and 3D transformations.
To find such vectors, a Gaussian scale-space is used.

Formally, a scale-space for a given 2D image $I(x, y)$ is defined as a function
$$L(x, y, \sigma) = G(x, y, \sigma) \times I(x, y)$$

The $\times$ symbol is a convolution of the two functions in $x$ and $y$, and $G$ is the Gaussian function
$$G(x, y, \sigma) = \frac{1}{2\pi \sigma^2} e^{-(x^2 + y^2) / 2\sigma^2}$$

This operation is then applied twice on image $I$ with $\sigma = \sqrt{2}$, yielding images $I_{1,1}, I_{2,1}$, the difference of which yields the image $G_1$.
After this, the image $I_{2,1}$ is downscaled by a certain factor (the original method uses $1.5$ with bilinear interpolation) and the process is repeated, yielding $I_{1,2}, I_{2,2}$ and $G_2$, respectively.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{images/gauss.png}
	\caption{An example of the convolution and difference applied to a hold image.}
\end{figure}

To find the features, the local minima and maxima (compared to the 8 neighbouring pixels) of $G_1$ are examined -- if they are also minima/maxima in $G_2$ (accounting for the downscaling), they are chosen as the key points.

The intuitive idea behind this approach is to suppress the finer details of the image, since each of the features in the coarse version of the image should be present in more detail in the original \cite{scalespace}.

To calculate the vectors for each of these key points, the magnitude $M(x,y)$ and orientation $R(x,y)$ is calculated using their neighbouring points via the following formula:
$$
\begin{aligned}
	M(x,y) &= \sqrt{\left(I_{1,1}(x, y) - I_{1,1}(x + 1, y)\right)^2 + \left(I_{1,1}(x,y) - I_{1,1}(x, y + 1)\right)^2} \\[0.7em]
	R(x,y) &= \mathrm{atan2} \left(I_{1,1}(x, y) - I_{1,1}(x + 1, y), I_{1,1}(x,y) - I_{1,1}(x, y + 1)\right)
\end{aligned}
$$

For the vectors to be invariant to orientation and contrast changes, a canonical orientation is calculated from their local image gradients (again using the Gaussian function with some parameter $\sigma$).

\subsection{Feature matching}

% SIFT still likely used here?
% These descriptors are used later to detect correspondences across the photos. This is similar to the well known SIFT approach, but uses different algorithms for a little bit higher alignment quality.

\subsection{Bundle adjustment}
After the features have been determined and matched across the images, the next step is to simultaneously calculate their position in the 3D space and also the positions of the cameras.
This is referred to as the bundle adjustment problem \cite{snavely2008modeling,schneider19913}, the name referring to bundles of rays from the predicted positions of the points going into the cameras.

Formally, we can model the scene by a vector of 3D points $\mathbf{X}_{i \in [n]}$, taken from cameras with parameters (position, focal length, etc.) given by the vector $\mathbf{P}_{j \in [m]}$.
Our matched features are observations $\underline{\bm{x}}_{ij}$ of point $i$ from camera $j$.

Let's now assume that we have a function $\bm{x}(\mathbf{X}_i, \mathbf{P}_j)$ that takes the feature position $\mathbf{X}_i$ and camera position $\mathbf{P}_j$, and returns the observation position $\mathbf{x}_{i, j}$.
Using this, the bundle adjustment problem can be formulated as a minimization problem of the function $$\Delta x_{i, j} (\mathbf{X}_i, \mathbf{M}_j) = \underline{\bm{x}}_{ij} - \bm{x}(\mathbf{X}_i, \mathbf{P}_j)$$
given the observed features and an appropriate cost function.

% TODO: non-linear least squares here

\subsection{Dense pointcloud}
To generate a high quality model, additional points have to be matched.
This is 

\subsection{Surface reconstruction}
% TODO: přečíst poissona
% \cite{kazhdan2006poisson}

\subsection{Texturing}
% TODO: ?

\section{Using markers}
In certain cases (such as this one), it is necessary for the generated model to be correctly scaled and positioned (in either local or world coordinates) such that it corresponds to its size in the real world.

A simple approach for local coordinates is to use easily recognizable markers placed around the scanned object.
Since their position in space is known (measured in advance), it can be used to infer the correct scale of the entire model.

\begin{figure}
	\centering
	\includesvg[width=0.7\columnwidth]{images/targets.svg}
	\caption{18th 14-bit marker (left) and the corresponding binary value (right). The grey segments denote the opposing pair of $1$ bits and the blue circle denotes the parity bit.}
\end{figure}

The type of the markers used in Metashape is based on \citet{schneider19913,borisPatent}.
Each circular target contains an inner and outer circle, with the inner serving the purpose of locating the marker in the images and the outer for encoding the marker data.

The exact number of stored bits depends on the size of the marker, most common beind $12$ and $14$ respectively.
The $0$ bits correspond to white segments and $1$ to black ones.
For decoding correctness and robustness, the following constraints are additionally imposed on each of the targets:

\begin{itemize}
	\item the targets must be rotationally invariant
	\item the first bit is a parity bit ($0$ for odd, $1$ for even)
	\item there must exist an opposing pair of $1$ bits
\end{itemize}

While this does reduce the number of viable targets (see \cref{tab:markers}), it is still more than enough for the purposes of scanning smaller objects (and thus for the porposes this paper).

\begin{table}
\centering\footnotesize\sf
\begin{tabular}{rrr}
\toprule
Bits & All targets & Valid targets \\
\midrule
12 & 4096   & 147 \\
14 & 16384  & 516 \\
16 & 65536  & 1861 \\
18 & 262144 & 6766 \\
\bottomrule
\end{tabular}
\caption{The number of targets, depending on the desired number of bits \cite{targetsPost}.}
\label{tab:markers}
\end{table}
