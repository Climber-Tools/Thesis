\chapter{Theory}\label{sec:theory}

\section{Feature-based photogrammetry}
Broadly speaking, photogrammetry is the process of obtaining information about objects (namely their model) using images.
Because the problem is difficult and involves a lot of steps and specialized algorithms, many open-source and proprietary programs have been developed to simplify this task.

The thesis uses the Agisoft Metashape software due to its highly configurable Python API and quality of generated models, with alternates being discussed in section \ref{sec:realization}.
Since it is closed-source, there is no easy way to describe and cite the exact algorithms used.
However, a forum statement from the lead developer Dmitry Semyonov \parencite{metashapeForumPost}, with addition to the license files for various open-source projects, outline the general methods used.

Starting with a set of images and ending with a textured model, the process can be split into the following parts:

\begin{enumerate}
	\item \textbf{feature extraction:} for each image, find features that are stable under linear and affine transformations, viewpoint change and illumination
	\item \textbf{feature matching:} find matching features across multiple images
	\item \textbf{bundle adjustment:} find the approximate positions of the camera and the matched features (obtaining „structure from motion“ data)
	\item \textbf{dense pointcloud generation:} obtain additional features (along with their normals) for higher model quality
	\item \textbf{surface reconstruction:} reconstruct the surface of the model
	\item \textbf{texturing:} create texture from the images on the constructed model
\end{enumerate}

The following sections aim to cover each of these in part.

\subsection{Feature extraction} \label{ch:fext}
The image features are extracted using the Scale Invariant Feature Transform (SIFT) method described in \citet{lowe1999object,lowe2004distinctive,snavely2008modeling}.
Each of the features is a vector describing some feature of the 2D image, invariant or partially invariant to linear and affine transformations, illuminance change and 3D transformations.
To find such vectors, a Gaussian scale-space is used.

Formally, a scale-space for a given 2D image $I(x, y)$ is defined as a function
\begin{equation} L(x, y, \sigma) = G(x, y, \sigma) \times I(x, y) \end{equation}

The $\times$ symbol is a convolution of the two functions in $x$ and $y$, and $G$ is the Gaussian function
\begin{equation}G(x, y, \sigma) = \frac{1}{2\pi \sigma^2} e^{-(x^2 + y^2) / 2\sigma^2}\end{equation}

This operation is applied twice on image $I$ with $\sigma = \sqrt{2}$, yielding images $I_{1,1}, I_{2,1}$, the difference of which yields the image $G_1$ (see figure \ref{fig:gaussexample}).
After this, the image $I_{2,1}$ is downscaled by a certain factor (the original method uses $1.5$ with bilinear interpolation) and the process is repeated, yielding $I_{1,2}, I_{2,2}$ and $G_2$, respectively.

% TODO: pořádně popsat parametry (this operation)

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{images/gauss.png}
	\caption{An example of the convolution and difference applied to a hold image.}
	\label{fig:gaussexample}
\end{figure}

To find the features, the local minima and maxima (compared to the 8 neighbouring pixels) of the pixels of $G_1$ are examined -- if they are also minima/maxima in $G_2$ (accounting for the downscaling), they are selected.
The intuitive idea behind this approach is to suppress the finer details of the image, since each of the features in the coarse version of the image should be present in more detail in the original \cite{scalespace}.

To calculate the vectors for the selected features, the magnitude $M(x,y)$ and orientation $R(x,y)$ is calculated using their neighbouring points via the following formula:
\begin{align}
	M(x,y) &= \sqrt{\left(I_{1,1}(x, y) - I_{1,1}(x + 1, y)\right)^2 + \left(I_{1,1}(x,y) - I_{1,1}(x, y + 1)\right)^2} \\[0.7em]
	R(x,y) &= \mathrm{atan2} \left(I_{1,1}(x, y) - I_{1,1}(x + 1, y), I_{1,1}(x,y) - I_{1,1}(x, y + 1)\right)
\end{align}

For the vectors to be invariant to orientation and contrast changes, a canonical representation is calculated from their local image gradients (again using the Gaussian function with some parameter $\sigma$).

\subsection{Feature matching}
Once the vectors in canonical representation have been calculated, the images are examined pairwise.
For each feature in one image, the nearest neighbour (in terms of the vector distance in a suitable norm) is found in the other.
To filter out features that only appear in one of the images, a limit is imposed on the ratio of the distances between the closest and the second closest neighbour (intuitively, correct matches should have one good candidate, not many).

Since the number of features to match can be quite large and no efficient exact algorithms exist for the problem of finding the nearest neighbour, an approximate algorithm called best-bin-first is used \cite{beis1997shape}, returning the nearest neighbour with a high probability, otherwise returning a close one.
The algorithm internally constructs a $k$-d tree to partition the space into bins of the feature vectors which it then searches in an efficient way.

\subsection{Bundle adjustment}
After the features have been determined and matched across the images, the next step is to simultaneously calculate their position in the 3D space and also the positions of the cameras.
This is referred to as the bundle adjustment problem \cite{snavely2008modeling,schneider19913}, the name referring to bundles of rays from the predicted positions of the points going into the cameras.

Formally, we can model the scene by vectors of 3D points $\mathbf{X}_{i \in [n]}$, taken from cameras with parameters (position, focal length, etc.) given by vectors $\mathbf{P}_{j \in [m]}$.
Our features can then be seen as observations $\underline{\bm{x}}_{ij}$ of point $i$ from camera $j$.

Let's now assume that we have a function $\bm{x}(\mathbf{X}_i, \mathbf{P}_j)$ that, given the feature position $\mathbf{X}_i$ and camera position $\mathbf{P}_j$, models the calculated observation position $\mathbf{x}_{i, j}$.
Given this function, the bundle adjustment problem can be formulated as a minimization problem of the function \begin{equation} \Delta x_{i, j} (\mathbf{X}_i, \mathbf{M}_j) = \underline{\bm{x}}_{ij} - \bm{x}(\mathbf{X}_i, \mathbf{P}_j) \end{equation}
over the observed features, and an appropriate cost function. For estimation using nonlinear least squares, the cost function to minimize is the weighted sum of squared error, formulated as 
\begin{equation} \frac{1}{2} \sum_{i,j} \Delta x_{i, j} (\mathbf{X}_i, \mathbf{M}_j)^T W_{i,j} \Delta x_{i, j} (\mathbf{X}_i, \mathbf{M}_j) \end{equation}
where each $W_i$ is a symmetric positive definite matrix indicating the weight of the vector, chosen to be approximately the inverse measured covariance of $\underline{\bm{x}}_{ij}$ (to account for the accuracy of measured feature points).

\subsection{Dense pointcloud generation}
Since the feature matching step generates a small number of features (to what would be considered a „high-quality mesh“), additional feature points have to be generated.
The approach is based on generating depth-maps (grayscale images indicating the distance of each pixel from the camera) for each image and merging them together \cite{shen2013accurate}.
It can be separated into four distinct parts.

\subsubsection{Stereo pair selection}
Each image is paired with another image to generate its depth-map.
The paired image should view the object from a similar direction but a different viewing angle that is not too close (\ang{5}) and too far (\ang{60}).
The camera centers should additionally be between $2 \overline{d}$ and $0.05 \overline{d}$, where $\overline{d}$ is the median of the distances between the neighbouring images.

The optimal image is selected as the one with the minimum of the viewing angle multiplied by the camera distance, with the others (sorted and limited to 10) are considered the image neighbourhood $N_i$.

\subsubsection{Depth-map computation}
The core of the computation is based on the PatchMatch algorithm \cite{barnes2009PAR}.

A random 3D point, along with a normal (thus forming a plane), is assigned to each of the pixels such that they are in the viewing ray of their camera.
Now for each of the pixels, a square „patch“ is placed on the pixel and for each pixel in it, a matching cost is calculated.

The pixels' matching cost is then iteratively improved either by moving its position to the plane of one of its neighbours, or by randomly changing it.

\subsubsection{Depth-map refinement}
To make the depth of the common areas consistent, each pixel of the depth-maps is considered.
If its corresponding 3D point is close to its position in the neighbouring images $N_i$ (when projecting it onto the neighbouring image and then back using its depth-map), it is retained, otherwise it is removed from the depth-map.

\subsubsection{Depth-map merging}
The calculated depth-maps for each image are merged by projecting them back to 3D (given the camera position) and combining them to form the resulting dense point cloud.
Duplicate depth-maps are filtered out by comparing their overlap with their neighbouring images.

\subsection{Surface reconstruction}
For surface reconstruction, the Poisson surface reconstruction algorithm \cite{kazhdan2006poisson} is used.
It takes points in 3D space and their normals (oriented towards the object), and constructs a function that represents the surface.

It does this by first computing a piecewise indicator function $\chi$ that is $0$ outside of the model and $1$ inside. The gradient of this indicator function (when convoluted with a suitable smoothing filter) is a vector field, whose values near the surface are the point normals.

The problem can thus be formulated as a minimization problem in terms of the difference between observed normals as a vector field (approximated by a discrete sum, since we only have a set of points and their normals), and the indicator function gradient:
\begin{equation} \min_\chi || \nabla \chi - \vec{V}|| \end{equation}

Least squares are again used to find the approximate solution.

\subsection{Texturing}
All that is left is to create a texture for the modeled object.
Since both the model and the camera position are known at this point, the images can be projected onto the faces in view (given that the face is pointed towards the camera).
Additional steps can be taken to normalize certain properties of the images such as the brightness, contrast, etc.

\section{Using markers}
In certain cases (such as this one), it is necessary for the generated model to be correctly scaled and positioned (in either local or world coordinates) such that it corresponds to its size in the real world.

A simple approach for local coordinates is to use easily recognizable markers placed around the scanned object.
Since their position in space is known (measured in advance), it can be used to infer the correct scale of the entire model.

\begin{figure}[h]
	\centering
	\includesvg[width=0.8\columnwidth]{images/targets.svg}
	\caption{18th 14-bit marker (left) and the corresponding binary value (right). Grey segment denotes an opposing pair of $1$ bits, blue circle denotes the parity bit.}
\end{figure}

The type of the markers used in Metashape is based on \citet{schneider19913,borisPatent}.
Each circular target contains an inner and outer circle, with the inner serving the purpose of locating the marker in the images and the outer for encoding the marker data.

The exact number of stored bits depends on the size of the marker, most common being $12$ and $14$ respectively.
The $0$ bits correspond to white segments and $1$ to black ones.
For decoding correctness and robustness, the following constraints are additionally imposed on the targets:

\begin{itemize}
	\item the targets must be rotationally invariant
	\item the first bit is a parity bit ($0$ for odd, $1$ for even)
	\item there must exist an opposing pair of $1$ bits
\end{itemize}

After the positions of the markers are determined in the 3D space (they are just a special type of features described in \ref{ch:fext}), it is just a matter of simple linear algebra to transform the position of other features appropriately.
